{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date:** Dec 17, 2024 \n",
    "**Author:** Revekka Gershovich\n",
    "**Purpose:** Align and merge ICPSR dataset (1834-1975) and Klarner dataset (1935-2011)\n",
    "**Preceded by Cleaning_icpsr16_partisan_composition.ipynb file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a different data source available from **Carl Klarner** for years 1935-2011. It can be found in raw_data_dir folder called Klarner_stateComposition. The folder contains two datasets. The one I load as klarner1 contains the actual data cleaned by Klarner while klarner2 contains data about the data sources from which the dataset was derived along with multiple parameters re: how various data sources recorded various data point. All of this is recorded in Word documents that are in the same folder. This documentation also explains how odd states are handled. \n",
    "\n",
    "Tha paper in the folder discusses problems such as legislature switches mid_session and biases that come from a bad measure of party control. We have to accept many of those problems due to the fact that they are only solved in this dataset after 1935 and we have to use a less reliable source for before then. \n",
    "\n",
    "### My next steps: \n",
    "1. Bring data to the format in which all the data is which involves renaming variables and computing measures of proportions of dems and reps in session, i.e. in both legislative chambers. \n",
    "2. Filtering both ICPRS and Klarner datasets to overlap years, i.e. 1935-1975, and merging those two datasets to see the discrepancies so as to resolve any or at least inconsistencies between the two datasets coding\n",
    "3. Dropping the years after 1935 from ICPSR dataset since it is less reliable of the sources, and appending Klarner dataset to it. \n",
    "4. I also have ncsl_state_composition data for years after 2011, that I will append after all that is done\n",
    "5. The final step would be to append data for all US governors that I downloaded from here: https://github.com/jacobkap/governors, and making sure that the rest of my data is consistent with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(\"/Users/revekkagershovich/Dropbox (MIT)/StateLaws\")\n",
    "os.chdir(parent_dir)\n",
    "assert os.path.exists(parent_dir), \"parent_dir does not exist\"\n",
    "intermed_data_dir = \"./2_data/2_intermediate/political_data\"\n",
    "assert os.path.exists(intermed_data_dir), \"Data directory does not exist\"\n",
    "raw_data_dir = \"./2_data/1_raw/political_data/all_partisanComposition\"\n",
    "assert os.path.exists(raw_data_dir), \"Data directory does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpsr = pd.read_csv(os.path.join(intermed_data_dir, \"icpsr.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Klarner Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keeping this cell so that you can easily load the data with info re: data sources\n",
    "\n",
    "# klarner2 = pd.read_excel(os.path.join(raw_data_dir, \"Klarner_partisan_composition/StatePartisanBalance1934to2011_SourceFiles_2011_05_24.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Karl Klarner's dataset for years 1934-2011\n",
    "klarner1_0 = pd.read_excel(os.path.join(raw_data_dir, \"Klarner_partisan_composition/Partisan_Balance_For_Use2011_06_09b.xlsx\"))\n",
    "\n",
    "# Define columns to check for missing data\n",
    "columns_to_check = [\n",
    "    'govparty_c', 'sen_dem_prop_all', 'sen_rep_prop_all', 'hs_dem_prop_all', 'hs_rep_prop_all',\n",
    "    'sen_dem_in_sess', 'sen_rep_in_sess', 'sen_tot_in_sess',\n",
    "    'hs_dem_in_sess', 'hs_rep_in_sess', 'hs_tot_in_sess'\n",
    "]\n",
    "\n",
    "# Create a boolean mask identifying rows where all `columns_to_check` are NA\n",
    "mask = klarner1_0[columns_to_check].isna().all(axis=1)\n",
    "\n",
    "identifiers = ['year', 'election_year', 'state', 'stateno', 'fips']\n",
    "\n",
    "# Create a boolean mask identifying rows where all columns other than identifiers are NA\n",
    "NA_mask = klarner1_0.loc[:, ~klarner1_0.columns.isin(identifiers)].isna().all(axis=1)\n",
    "\n",
    "# Identify rows where all columns other than identifiers are NA\n",
    "NaN_data = klarner1_0[mask]\n",
    "\n",
    "# Inspect the distribution of `election_year` for NaN data\n",
    "print(\"Data for how many states is missing each year?\")\n",
    "print(NaN_data['election_year'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output it is clear that for years 2011-2014 and 1934, all states formally exist in the data but all variables of interest apart from identificator columns are missing. \n",
    "Most data also seems to be missing for years 1934 and 1935 (or not all states were present in the data to begin with). And for years 1936-1947 data for two states is missing. I will now find out for which states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define years of interest to filter rows to drop\n",
    "years_of_interest = [1942, 1946, 1945, 1944, 1943, 1938, 1941, 1940, 1939, 1937, 1936, 1947]\n",
    "\n",
    "# Filter rows to drop for the specific years of interest\n",
    "filtered_rows = NaN_data[NaN_data['election_year'].isin(years_of_interest)]\n",
    "\n",
    "# Group by `election_year` and list unique states for each year\n",
    "states_by_year = filtered_rows.groupby('election_year')['state'].unique()\n",
    "\n",
    "# Display the states grouped by `election_year`\n",
    "print(states_by_year)  \n",
    "# Alaska and Hawaii data are missing for years 1936-1947 because they joined in 1959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove rows where all `columns_to_check` are NA from `klarner1_0`\n",
    "klarner_noNAs = klarner1_0[~mask]\n",
    "\n",
    "# Print the count of `election_year` values, ordered by year\n",
    "print(\"Number of states in data for each election year:\")\n",
    "print(klarner_noNAs['election_year'].value_counts().sort_index())\n",
    "\n",
    "# When we drop all columns with NAs in non_dentification columns,\n",
    "# we drop all observations before 1935 and after 2010, and in 1935 we only have five states.\n",
    "\n",
    "# Filter data for 1935 and list unique states\n",
    "states_1935 = klarner_noNAs[klarner_noNAs['election_year'] == 1935]['state'].unique()\n",
    "\n",
    "# Display the result\n",
    "print(states_1935)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the columns that are needed for the analysis\n",
    "klarner1 = klarner_noNAs[['state', 'election_year', 'sen_dem_prop_all', 'sen_rep_prop_all', 'hs_dem_prop_all', \n",
    "'hs_rep_prop_all', 'sen_dem_in_sess', 'sen_rep_in_sess', 'sen_tot_in_sess', 'hs_dem_in_sess', \n",
    "'hs_rep_in_sess', 'hs_tot_in_sess', 'govparty_c']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Dictionary mapping state names to abbreviations\n",
    "state_to_abbrev = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n",
    "    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n",
    "    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
    "    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO', 'Montana': 'MT',\n",
    "    'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', \n",
    "    'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK', \n",
    "    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC', \n",
    "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', \n",
    "    'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'}\n",
    "\n",
    "# Add a new column to the DataFrame with the abbreviations\n",
    "klarner1.loc[:, 'state_abbrev'] = klarner1['state'].map(state_to_abbrev)\n",
    "\n",
    "# Since in ICPSR I only have state abbreviations, I will drop the column containing the full state names\n",
    "klarner1 = klarner1.drop(columns=['state'])\n",
    "\n",
    "# Rename the columns to match the ICPSR dataset\n",
    "klarner1 = klarner1.rename(columns={\n",
    "    'election_year': 'year',\n",
    "    'govparty_c': 'gov_party',\n",
    "    'sen_dem_prop_all': 'dem_upphse',\n",
    "    'sen_rep_prop_all': 'rep_upphse',\n",
    "    'hs_dem_prop_all': 'dem_lowhse',\n",
    "    'hs_rep_prop_all': 'rep_lowhse',\n",
    "})\n",
    "\n",
    "# Keeping only the final dataset columns and the columns needed to calculate the share of Democrats and Republicans in the session\n",
    "klarner1 = klarner1[[\n",
    "    'year', 'state_abbrev', 'gov_party',\n",
    "    'dem_upphse', 'rep_upphse', 'dem_lowhse', 'rep_lowhse',\n",
    "    'sen_dem_in_sess', 'sen_rep_in_sess', 'sen_tot_in_sess',\n",
    "    'hs_dem_in_sess', 'hs_rep_in_sess', 'hs_tot_in_sess'\n",
    "]]\n",
    "\n",
    "# Calculate the overall share of Democratic and Republican seats in the session (upper + lower house) - this measure is not available in the dataset\n",
    "klarner1['shr_dem_in_sess'] = (klarner1['sen_dem_in_sess'] + klarner1['hs_dem_in_sess']) / (klarner1['hs_tot_in_sess'] + klarner1['sen_tot_in_sess'])\n",
    "klarner1['shr_rep_in_sess'] = (klarner1['sen_rep_in_sess'] + klarner1['hs_rep_in_sess']) / (klarner1['hs_tot_in_sess'] + klarner1['sen_tot_in_sess'])\n",
    "\n",
    "# Drop the columns that were used for calculating overall share of dem/rep seats in the session and are not needed anymore\n",
    "klarner1 = klarner1.drop(columns=['sen_dem_in_sess', 'sen_rep_in_sess', 'sen_tot_in_sess', 'hs_dem_in_sess', 'hs_rep_in_sess', 'hs_tot_in_sess'])\n",
    "\n",
    "# In the Klarner dataset democrats are coded as 1 like I map ICPSR data, but Republicans are coded as 0, not 2. \n",
    "# I will change this to match ICPSR data. Value 0.5 signify non-major party governor, however I was just \n",
    "# dropping those values in ICPSR data, and they are automatically dropped with the mapping.\n",
    "klarner1['gov_party'] = klarner1['gov_party'].map({1.0: 1, 0.0: 2})\n",
    "\n",
    "print(klarner1.sample(5, random_state = 44))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klarner1['year'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique states\n",
    "print(f\"Number of unique states: {klarner1['state_abbrev'].nunique()}\")\n",
    "\n",
    "# Assertions\n",
    "assert klarner1['state_abbrev'].nunique() == 50, \"There should be 50 states.\"\n",
    "\n",
    "assert klarner1['year'].min() == 1935, \"The minimum year should be 1935.\"\n",
    "assert klarner1['year'].max() == 2010, \"The maximum year should be 2010 because the election year should be off-set one year back.\"\n",
    "assert klarner1['year'].nunique() == 76, \"There should be 76 unique years.\"\n",
    "\n",
    "assert klarner1['gov_party'].nunique() == 2, \"There should be 2 unique parties.\"\n",
    "\n",
    "# Check if all values in 'gov_party' are valid\n",
    "valid_values = {1, 2}  \n",
    "assert klarner1['gov_party'].dropna().isin(valid_values).all(), \"All values in 'gov_party' should be 1, 2, or NaN.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that Klarner data is cleaned and brought to the standart format, I will filter ICPSR and Klarner data to contain only the years that are in both datasets, i.e. 1935-1975. The idea is to make sure that this data is the same in both datasets, and then apply the coding to the rest of the data to make sure that it is consistent across all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing ICPSR and Klarner Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ICPSR and Klarner data to keep only years in both datasets, i.e. 1935-1975\n",
    "icpsr_filt = icpsr[icpsr['year'] >= 1935]\n",
    "klarner_filt = klarner1[klarner1['year'] <= 1975]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing datasets for even years only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering both datasets only for even years to see if they align for the even years \n",
    "# at least since odd yeats are more tricky.\n",
    "icpsr_filt_even = icpsr_filt[icpsr_filt['year'] % 2 == 0]\n",
    "klarner_filt_even = klarner_filt[klarner_filt['year'] % 2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging datasets only for even years to be able to compare the data from both sources\n",
    "merged_even = pd.merge(klarner_filt_even, icpsr_filt_even, \n",
    "on=['year', 'state_abbrev'], suffixes=('_klarner', '_icpsr'), how='outer')\n",
    "\n",
    "# Reorder the columns\n",
    "merged_even = merged_even[['year', 'state_abbrev', 'gov_party_klarner', 'gov_party_icpsr',\n",
    "                           'dem_upphse_klarner', 'dem_upphse_icpsr', 'rep_upphse_klarner', \n",
    "                           'rep_upphse_icpsr', 'dem_lowhse_klarner', 'dem_lowhse_icpsr', \n",
    "                           'rep_lowhse_klarner', 'rep_lowhse_icpsr', 'shr_dem_in_sess_klarner', \n",
    "                           'shr_dem_in_sess_icpsr', 'shr_rep_in_sess_klarner', 'shr_rep_in_sess_icpsr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unequal_rows_gov_even = merged_even[\n",
    "    merged_even['gov_party_klarner'].notna() & \n",
    "    merged_even['gov_party_icpsr'].notna() & \n",
    "    (merged_even['gov_party_klarner'] != merged_even['gov_party_icpsr'])\n",
    "]\n",
    "print(f\"There are {unequal_rows_gov_even.shape[0]} rows where 'gov_party_klarner' is not equal to 'gov_party_icpsr'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unequal_rows_gov_even[['year', 'state_abbrev', 'gov_party_klarner', 'gov_party_icpsr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a dataset to double-check the problematic years with links to sources generated by ChatGPT\n",
    "# Create the DataFrame\n",
    "data = {\n",
    "    \"year\": [1938, 1942, 1948, 1950, 1954, 1958, 1964, 1966, 1968, 1968],\n",
    "    \"state_abbrev\": [\"OH\", \"NY\", \"WY\", \"CO\", \"NY\", \"NY\", \"UT\", \"UT\", \"MD\", \"UT\"],\n",
    "    \"governor\": [\n",
    "        \"John W. Bricker\", \"Thomas E. Dewey\", \"Arthur G. Crane\", \"Daniel I.J. Thornton\",\n",
    "        \"W. Averell Harriman\", \"Nelson A. Rockefeller\", \"Cal Rampton\", \"Cal Rampton\",\n",
    "        \"Marvin Mandel\", \"Cal Rampton\"\n",
    "    ],\n",
    "    \"party_code\": [2, 2, 2, 2, 1, 2, 1, 1, 1, 1],\n",
    "    \"source_url\": [\n",
    "        \"https://en.wikipedia.org/wiki/1938_Ohio_gubernatorial_election\",\n",
    "        \"https://en.wikipedia.org/wiki/1942_New_York_state_election\",\n",
    "        \"https://en.wikipedia.org/wiki/List_of_governors_of_Wyoming\",\n",
    "        \"https://en.wikipedia.org/wiki/1950_Colorado_gubernatorial_election\",\n",
    "        \"https://en.wikipedia.org/wiki/1954_New_York_state_election\",\n",
    "        \"https://en.wikipedia.org/wiki/1958_New_York_state_election\",\n",
    "        \"https://en.wikipedia.org/wiki/1964_Utah_gubernatorial_election\",\n",
    "        \"https://en.wikipedia.org/wiki/List_of_governors_of_Utah\",\n",
    "        \"https://en.wikipedia.org/wiki/List_of_governors_of_Maryland\",\n",
    "        \"https://en.wikipedia.org/wiki/List_of_governors_of_Utah\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "gov_mismatch = pd.DataFrame(data)\n",
    "\n",
    "# Perform an inner merge to compare rows based on 'year' and 'state_abbrev'\n",
    "comparison = unequal_rows_gov_even.merge(gov_mismatch, on=['year', 'state_abbrev'], how='inner')\n",
    "\n",
    "# Identify mismatched rows where 'gov_party_klarner' does not match 'party_code'\n",
    "mismatched_rows_klarner = comparison[comparison['gov_party_klarner'] != comparison['party_code']]\n",
    "\n",
    "# Display the mismatched rows\n",
    "print(f\"Number of rows that are mismatched between Klarner and the correct data: {mismatched_rows_klarner.shape[0]}\")\n",
    "\n",
    "# Identify mismatched rows where 'gov_party_icpsr' does not match 'party_code'\n",
    "mismatched_rows_icpsr = comparison[comparison['gov_party_icpsr'] != comparison['party_code']]\n",
    "\n",
    "# Display the mismatched rows\n",
    "print(f\"Number of rows that are mismatched between ICPSR and the correct data: {mismatched_rows_icpsr.shape[0]}\")\n",
    "\n",
    "print(\"Thus it is clear that Klarner is the more accurate dataset, and all the instances of mismatch are due to errors in ICPSR data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows where 'dem_upphse_klarner' and 'dem_upphse_icpsr' are not close, excluding rows where both are NaN\n",
    "unequal_rows_dem_upphse = merged_even[\n",
    "    ~np.isclose(merged_even['dem_upphse_klarner'], merged_even['dem_upphse_icpsr'], atol=0.05) &\n",
    "    ~(merged_even['dem_upphse_klarner'].isna() & merged_even['dem_upphse_icpsr'].isna())\n",
    "]\n",
    "\n",
    "unequal_rows_dem_upphse = unequal_rows_dem_upphse[['year', 'state_abbrev', 'dem_upphse_klarner', 'dem_upphse_icpsr']]\n",
    "\n",
    "# Print the number of rows where the values are not equal\n",
    "print(f\"There are {unequal_rows_dem_upphse.shape[0]} rows where 'dem_upphse_klarner' is not equal to 'dem_upphse_icpsr'.\")\n",
    "\n",
    "# Display the mismatched rows\n",
    "print(unequal_rows_dem_upphse.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that out of 6 states for which mismatch occurs most often, \n",
    "# five states have odd-year state elections, i.e. MA, VA, KY, NJ, and LA which \n",
    "# suggests that they are coded differently in the two datasets.\n",
    "unequal_rows_dem_upphse['state_abbrev'].value_counts().head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unequal_rows_dem_upphse_odd_states = unequal_rows_dem_upphse[unequal_rows_dem_upphse['state_abbrev'].isin(['MS', 'VA', 'KY', 'NJ', 'LA'])]\n",
    "\n",
    "# # Display the mismatched rows for odd-year state elections\n",
    "# print(unequal_rows_dem_upphse_odd_states)\n",
    "\n",
    "print(unequal_rows_dem_upphse_odd_states['dem_upphse_icpsr'].value_counts(dropna = False))\n",
    "# We can see that the ICPSR data has a lot of missing values for these states in eveh years where Klarner does not meaning \n",
    "# in ICPSR the data is only available for the actual election years.\n",
    "\n",
    "print(unequal_rows_dem_upphse_odd_states.dropna()) \n",
    "# In the remaining rows it seems like ICPSR values are just rounded up Klarner values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unequal_rows_dem_upphse_only_even_states = unequal_rows_dem_upphse[~unequal_rows_dem_upphse['state_abbrev'].isin(['MS', 'VA', 'KY', 'NJ', 'LA'])]\n",
    "\n",
    "# Display the mismatched rows for even-year state elections\n",
    "print(unequal_rows_dem_upphse_only_even_states.head())\n",
    "\n",
    "# Mostly those differences are small, and are probably caused by adjustments in the Klarner dataset or mistakes/missing data in icpsr data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and print rows where 'dem_upphse_klarner' is NA\n",
    "print(unequal_rows_dem_upphse_only_even_states[unequal_rows_dem_upphse_only_even_states['dem_upphse_klarner'].isna()])\n",
    "\n",
    "merged_even[(merged_even['state_abbrev'] == 'AL') & (merged_even['year'] == 1936)]\n",
    "\n",
    "# There is missing data for Alabama in 1936 in the Klarner dataset, \n",
    "# and the ICPSR dataset has a value of 0.0 for the Democratic proportion in the upper house which is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude rows already in unequal_rows_dem_upphse\n",
    "remaining_rows = merged_even.loc[~merged_even.index.isin(unequal_rows_dem_upphse.index)]\n",
    "\n",
    "# Identify rows where 'rep_upphse_klarner' and 'rep_upphse_icpsr' are not close, excluding NaN matches\n",
    "unequal_rows_rep_upphse = remaining_rows[\n",
    "    ~np.isclose(remaining_rows['rep_upphse_klarner'], remaining_rows['rep_upphse_icpsr'], atol=0.1) &\n",
    "    ~(remaining_rows['rep_upphse_klarner'].isna() & remaining_rows['rep_upphse_icpsr'].isna())\n",
    "]\n",
    "\n",
    "# Display the results\n",
    "print(f\"There are {unequal_rows_rep_upphse.shape[0]} rows where 'rep_upphse_klarner' is not equal to 'rep_upphse_icpsr'.\")\n",
    "print(unequal_rows_rep_upphse[['year', 'state_abbrev', 'rep_upphse_klarner', 'rep_upphse_icpsr']])\n",
    "# i double-ckecked that Klarner data is correct for the row with a mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unequal_rows_dem_lowhse = remaining_rows[\n",
    "    ~np.isclose(remaining_rows['dem_lowhse_klarner'], remaining_rows['dem_lowhse_icpsr'], atol=0.1) &\n",
    "    ~(remaining_rows['dem_lowhse_klarner'].isna() & remaining_rows['dem_lowhse_icpsr'].isna())\n",
    "]\n",
    "\n",
    "# Display the results\n",
    "print(f\"There are {unequal_rows_dem_lowhse.shape[0]} rows where 'rep_upphse_klarner' is not equal to 'rep_upphse_icpsr'.\")\n",
    "print(unequal_rows_dem_lowhse[['year', 'state_abbrev', 'dem_lowhse_klarner', 'dem_lowhse_icpsr']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Filling 1935-1975 ICPSR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(icpsr_filt['year'].value_counts().sort_index().head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is only available for the actual election years, and not for the odd years in between (or even years for states on odd-year election cycle states). I will forward-fill the ICPSR dataset for each state for each year where data is not available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete grid of years and states\n",
    "years = icpsr_filt['year'].unique()\n",
    "states = icpsr_filt['state_abbrev'].unique()\n",
    "\n",
    "# Create a DataFrame with all combinations\n",
    "all_combos = pd.MultiIndex.from_product([years, states], names=['year', 'state_abbrev']).to_frame(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the complete grid with the original dataset\n",
    "icpsr_complete = pd.merge(all_combos, icpsr_filt, on=['year', 'state_abbrev'], how='left')\n",
    "\n",
    "print(\n",
    "    icpsr_complete[icpsr_complete['state_abbrev'] == 'AZ']\n",
    "    [['year', 'state_abbrev', 'gov_party', 'rep_upphse']]\n",
    "    .sort_values(by='year', ascending=True)\n",
    "    .head(3)\n",
    ")\n",
    "\n",
    "# Identify identifier columns (e.g., year and state_abbrev)\n",
    "id_cols = ['year', 'state_abbrev']\n",
    "gov_id_cols = ['year', 'state_abbrev', 'gov_party']\n",
    "\n",
    "# Identify non-identifier columns\n",
    "non_id_cols = [col for col in icpsr_complete.columns if col not in id_cols]\n",
    "non_gov_id_cols = [col for col in icpsr_complete.columns if col not in gov_id_cols]\n",
    "\n",
    "# Forward-fill for each state\n",
    "for state in states:\n",
    "    # Subset the data for the current state and sort by year\n",
    "    state_data = icpsr_complete[icpsr_complete['state_abbrev'] == state].sort_values(by='year')\n",
    "    \n",
    "    # Forward-fill non-identifier columns\n",
    "    state_data[non_id_cols] = state_data[non_id_cols].ffill()\n",
    "    \n",
    "    # Forward-fill non-gov_party columns\n",
    "    state_data[non_gov_id_cols] = state_data[non_gov_id_cols].ffill()\n",
    "    \n",
    "    # Update the main DataFrame\n",
    "    icpsr_complete.loc[state_data.index, non_id_cols] = state_data[non_id_cols]\n",
    "    icpsr_complete.loc[state_data.index, non_gov_id_cols] = state_data[non_gov_id_cols]\n",
    "\n",
    "# Display results for AZ\n",
    "print(\n",
    "    icpsr_complete[icpsr_complete['state_abbrev'] == 'AZ']\n",
    "    [['year', 'state_abbrev', 'gov_party', 'rep_upphse']]\n",
    "    .sort_values(by='year', ascending=True)\n",
    "    .head(3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if after forward-filling ICPSR, Klarner and ICPSR data are aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging klarner_filt which a dataset for the same years as icpsr_complete to see if the data aligns\n",
    "merged = pd.merge(klarner_filt, icpsr_complete, on=['year', 'state_abbrev'], suffixes=('_klarner', '_icpsr_comp'), how='outer')\n",
    "merged = pd.merge(merged, icpsr_filt, on=['year', 'state_abbrev'], suffixes=('_klarner', '_icpsr_filt'), how='outer')\n",
    "\n",
    "print(merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows where 'gov_party_klarner' is not equal to 'gov_party_icpsr'\n",
    "unequal_rows_gov = merged[\n",
    "    merged['gov_party_klarner'].notna() &\n",
    "    merged['gov_party'].notna() &\n",
    "    (merged['gov_party_klarner'] != merged['gov_party_icpsr_comp'])\n",
    "]\n",
    "\n",
    "print(f\"There are {unequal_rows_gov.shape[0]} rows where 'gov_party_klarner' is not equal to 'gov_party'.\")\n",
    "\n",
    "# print(unequal_rows_gov)\n",
    "print(unequal_rows_gov['year'].value_counts().sort_index().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unequal_rows_gov_ev = unequal_rows_gov[unequal_rows_gov['year'] % 2 == 0]\n",
    "assert unequal_rows_gov_ev.shape[0] == 10, \"There should be 10 rows because that's the number of mismatched observations before forward-filling.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unequal_rows_gov = merged[\n",
    "    merged['gov_party_klarner'].notna() &\n",
    "    merged['gov_party_icpsr_comp'].notna() &\n",
    "    merged['gov_party'].isna() &\n",
    "    (merged['gov_party_klarner'] != merged['gov_party_icpsr_comp'])\n",
    "]\n",
    "\n",
    "print(f\"There are {unequal_rows_gov.shape[0]} rows where 'gov_party_klarner' is not equal to 'gov_party_icpsr_comp'.\")\n",
    "\n",
    "print(unequal_rows_gov['state_abbrev'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged['state_abbrev'] == 'NE'][['year', 'state_abbrev', 'gov_party_klarner', 'gov_party_icpsr_comp', 'gov_party']].head(10)\n",
    "# unequal_rows_gov[['year', 'state_abbrev', 'gov_party_klarner', 'gov_party_icpsr_comp', 'gov_party']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the worst state is Nebraska because in Nebraska legislature is officially nonpartisan.Candidates for the Nebraska Legislature and governors appear on the ballot without party labels during both primary and general elections. One can still deduce what party affiliation they have, and Klarner dataset does. This is not really a problem at all since Nebraska legislature became officially nonpartisan in 1937, i.e. after Klarner data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unequal_rows_gov = merged[\n",
    "    merged['gov_party_klarner'].notna() &\n",
    "    merged['gov_party_icpsr_comp'].notna() &\n",
    "    (merged['gov_party_klarner'] != merged['gov_party_icpsr_comp'])\n",
    "]\n",
    "\n",
    "unequal_rows_gov = unequal_rows_gov[\n",
    "    (unequal_rows_gov['year'] != 1935) & \n",
    "    (unequal_rows_gov['year'] != 1939) & \n",
    "    ~unequal_rows_gov['year'].between(1970, 1975) &\n",
    "    (unequal_rows_gov['state_abbrev'] != 'NE')\n",
    "]\n",
    "\n",
    "print(unequal_rows_gov.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unequal_rows_gov['year'].value_counts().head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unequal_rows_gov[(unequal_rows_gov['year'] > 1971) & (unequal_rows_gov['year'] < 1976)][['year', 'state_abbrev', 'gov_party_klarner', 'gov_party_icpsr_comp', 'gov_party']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unequal_rows_gov[(unequal_rows_gov['year'] > 1971) & (unequal_rows_gov['year'] < 1976) & (unequal_rows_gov['gov_party'].notna())][['year', 'state_abbrev', 'gov_party_klarner', 'gov_party_icpsr_comp', 'gov_party']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpsr[(icpsr['year'] > 1936) & (icpsr['gov_party'].isna())]['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpsr[(icpsr['year'] < 1933) & (icpsr['gov_party'].isna())]['year'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(klarner_filt, icpsr_complete, on=['year', 'state_abbrev'], suffixes=('_klarner', '_icpsr'), how='outer')\n",
    "\n",
    "# Identify all columns with '_klarner' and '_icpsr' suffixes\n",
    "klarner_cols = [col for col in merged.columns if col.endswith('_klarner')]\n",
    "icpsr_cols = [col.replace('_klarner', '_icpsr') for col in klarner_cols if col.replace('_klarner', '_icpsr') in merged.columns]\n",
    "\n",
    "# Initialize a dictionary to store comparison results\n",
    "comparison_results = {}\n",
    "\n",
    "# Define the error margin\n",
    "error_margin = 0.01\n",
    "\n",
    "# Compare each pair of _klarner and _icpsr columns\n",
    "for klarner_col, icpsr_col in zip(klarner_cols, icpsr_cols):\n",
    "    # Check for mismatched rows within the error margin\n",
    "    mismatched_rows = merged[~np.isclose(merged[klarner_col], merged[icpsr_col], atol=error_margin)]\n",
    "    \n",
    "    # Exclude rows where both are NaN\n",
    "    mismatched_rows = mismatched_rows[\n",
    "        ~(merged[klarner_col].isna() & merged[icpsr_col].isna())\n",
    "    ]\n",
    "    \n",
    "    # Store the mismatches in the dictionary\n",
    "    comparison_results[f\"{klarner_col} vs {icpsr_col}\"] = mismatched_rows[['year', 'state_abbrev', klarner_col, icpsr_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At 0.01 error margin, the percentage of mismatches is less than 20% for all columns. \n",
    "# Not fantastic but passable given the quality of ICPSR data, and the fact that all errors \n",
    "# are propagated by forward-filling.\n",
    "col_types = ['gov_party', 'dem_upphse', 'rep_upphse', 'dem_lowhse', 'rep_lowhse', 'shr_dem_in_sess', 'shr_rep_in_sess']\n",
    "mismatch_percentages = []  # To store mismatch percentages\n",
    "\n",
    "for col_type in col_types:\n",
    "    mismatched_data = comparison_results[f'{col_type}_klarner vs {col_type}_icpsr']\n",
    "    percentage_mismatch = mismatched_data.shape[0] / merged.shape[0] * 100\n",
    "    mismatch_percentages.append(percentage_mismatch)\n",
    "    print(f\"Percentage of mismatches At 0.01 error margin:\")\n",
    "    print(f\"There are {mismatched_data.shape[0]} {col_type} mismatches.\")\n",
    "    print(f\"So for {col_type} there are {percentage_mismatch:.1f}% of data mismatched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(col_types, mismatch_percentages, color='skyblue')\n",
    "plt.title(\"Percentage of Mismatches at 0.01 Error Margin\", fontsize=14)\n",
    "plt.xlabel(\"Column Types\", fontsize=12)\n",
    "plt.ylabel(\"Mismatch Percentage (%)\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the error margin\n",
    "error_margin = 0.1\n",
    "\n",
    "# Compare each pair of _klarner and _icpsr columns\n",
    "for klarner_col, icpsr_col in zip(klarner_cols, icpsr_cols):\n",
    "    # Check for mismatched rows within the error margin\n",
    "    mismatched_rows = merged[~np.isclose(merged[klarner_col], merged[icpsr_col], atol=error_margin)]\n",
    "    \n",
    "    # Exclude rows where both are NaN\n",
    "    mismatched_rows = mismatched_rows[\n",
    "        ~(merged[klarner_col].isna() & merged[icpsr_col].isna())\n",
    "    ]\n",
    "    \n",
    "    # Store the mismatches in the dictionary\n",
    "    comparison_results[f\"{klarner_col} vs {icpsr_col}\"] = mismatched_rows[['year', 'state_abbrev', klarner_col, icpsr_col]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At 0.01 error margin, the percentage of mismatches is less than 20% for all columns. \n",
    "# Not fantastic but passable given the quality of ICPSR data, and the fact that all errors \n",
    "# are propagated by forward-filling.\n",
    "col_types = ['gov_party', 'dem_upphse', 'rep_upphse', 'dem_lowhse', 'rep_lowhse', 'shr_dem_in_sess', 'shr_rep_in_sess']\n",
    "mismatch_percentages = []  # To store mismatch percentages\n",
    "\n",
    "for col_type in col_types:\n",
    "    mismatched_data = comparison_results[f'{col_type}_klarner vs {col_type}_icpsr']\n",
    "    percentage_mismatch = mismatched_data.shape[0] / merged.shape[0] * 100\n",
    "    mismatch_percentages.append(percentage_mismatch)\n",
    "    print(f\"Percentage of mismatches At 0.01 error margin:\")\n",
    "    print(f\"There are {mismatched_data.shape[0]} {col_type} mismatches.\")\n",
    "    print(f\"So for {col_type} there are {percentage_mismatch:.1f}% of data mismatched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(col_types, mismatch_percentages, color='skyblue')\n",
    "plt.title(\"Percentage of Mismatches at 0.1 Error Margin\", fontsize=14)\n",
    "plt.xlabel(\"Column Types\", fontsize=12)\n",
    "plt.ylabel(\"Mismatch Percentage (%)\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, we can see that the forward-filling works but it propagates missing values and errors in the data of which there are plenty in the ICPSR 16 dataset. I will use separate governor's data to ensure accuracy for governors. I will use more accurate Klarner dataset for after 1935. It's not ideal but it's the best we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward filling ICPSR data for before 1935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpsr_filt = icpsr[icpsr['year'] <= 1935]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete grid of years and states\n",
    "years = icpsr_filt['year'].unique()\n",
    "states = icpsr_filt['state_abbrev'].unique()\n",
    "\n",
    "# Create a DataFrame with all combinations\n",
    "all_combos = pd.MultiIndex.from_product([years, states], names=['year', 'state_abbrev']).to_frame(index=False)\n",
    "\n",
    "print(all_combos.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statehood_df = pd.read_csv(os.path.join(raw_data_dir, \"statehood\", \"statehood_data.csv\"))\n",
    "\n",
    "# Extract the year from 'date_entered' and store it in a new column 'year'\n",
    "statehood_df['statehood_year'] = statehood_df['date_entered'].str.extract(r'(\\d{4})').astype(int)\n",
    "\n",
    "# Rename 'abbr' to 'state_abbrev'\n",
    "statehood_df = statehood_df.rename(columns={'abbr': 'state_abbrev'})\n",
    "\n",
    "# Keep only 'year' and 'state_abbrev' columns\n",
    "statehood_df = statehood_df[['statehood_year', 'state_abbrev']]\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(statehood_df.head())\n",
    "\n",
    "statehood_df2 = statehood_df.copy()\n",
    "\n",
    "statehood_df2['statehood_year'] = statehood_df2['statehood_year'] - 1\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(statehood_df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the statehood data to filter by valid years\n",
    "all_combos = all_combos.merge(statehood_df2, on='state_abbrev', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where the year is greater than or equal to the statehood year\n",
    "all_combos = all_combos[all_combos['year'] >= all_combos['statehood_year']].drop(columns=['statehood_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the complete grid with the original dataset\n",
    "icpsr_complete = pd.merge(all_combos, icpsr_filt, on=['year', 'state_abbrev'], how='left')\n",
    "\n",
    "print(\n",
    "    icpsr_complete[icpsr_complete['state_abbrev'] == 'AZ']\n",
    "    [['year', 'state_abbrev', 'gov_party', 'rep_upphse']]\n",
    "    .sort_values(by='year', ascending=True)\n",
    "    .head()\n",
    ")\n",
    "\n",
    "# Identify identifier columns (e.g., year and state_abbrev)\n",
    "id_cols = ['year', 'state_abbrev']\n",
    "gov_id_cols = ['year', 'state_abbrev', 'gov_party']\n",
    "\n",
    "# Identify non-identifier columns\n",
    "non_id_cols = [col for col in icpsr_complete.columns if col not in id_cols]\n",
    "non_gov_id_cols = [col for col in icpsr_complete.columns if col not in gov_id_cols]\n",
    "\n",
    "# Forward-fill for each state\n",
    "for state in states:\n",
    "    # Subset the data for the current state and sort by year\n",
    "    state_data = icpsr_complete[icpsr_complete['state_abbrev'] == state].sort_values(by='year')\n",
    "    \n",
    "    # Forward-fill non-identifier columns\n",
    "    state_data[non_id_cols] = state_data[non_id_cols].ffill()\n",
    "    \n",
    "    # Forward-fill non-gov_party columns\n",
    "    state_data[non_gov_id_cols] = state_data[non_gov_id_cols].ffill()\n",
    "    \n",
    "    # Update the main DataFrame\n",
    "    icpsr_complete.loc[state_data.index, non_id_cols] = state_data[non_id_cols]\n",
    "    icpsr_complete.loc[state_data.index, non_gov_id_cols] = state_data[non_gov_id_cols]\n",
    "\n",
    "# Display results for AZ\n",
    "print(\n",
    "    icpsr_complete[icpsr_complete['state_abbrev'] == 'AZ']\n",
    "    [['year', 'state_abbrev', 'gov_party', 'rep_upphse']]\n",
    "    .sort_values(by='year', ascending=True)\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertions\n",
    "assert icpsr_complete['state_abbrev'].nunique() == 48, \"There should be 48 states because the data is before 1935.\"\n",
    "assert icpsr_complete['year'].min() == 1834, \"The minimum year should be 1834.\"\n",
    "assert icpsr_complete['year'].max() == 1935, \"The maximum year should be 1935.\"\n",
    "assert icpsr_complete['year'].nunique() == 102, \"There should be 102 unique years.\"\n",
    "\n",
    "assert icpsr_complete['gov_party'].nunique() == 2, \"There should be 2 unique parties.\"\n",
    "\n",
    "# Check if all values in 'gov_party' are valid\n",
    "valid_values = {1, 2}  \n",
    "assert icpsr_complete['gov_party'].dropna().isin(valid_values).all(), \"All values in 'gov_party' should be 1, 2, or NaN.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading ncsl_state_composition data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF is the only format in which the data is available on ncsl website. I first extracted those files into excel format using Adobe Pdf Extraction tool which can be found here: https://www.adobe.com/au/acrobat/roc/blog/how-to-convert-pdf-to-csv.html#:~:text=Adobe%20Acrobat%20online%20services The free tool does not allow bulk extraction. MIT account gives access to upgraded tool with extraction. I then removed the first row manually and everything after \"TOTAL\" field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path where CSV files are stored\n",
    "folder_path = os.path.join(raw_data_dir, \"ncsl_statepartisancomposition\")\n",
    "assert os.path.exists(folder_path), \"Folder path is incorrect\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "xlsx_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.xlsx') and not f.startswith('~$')])\n",
    "\n",
    "# Efficiently load and concatenate all excel files\n",
    "ncsl_dfs = []  # List to hold individual DataFrames\n",
    "for file in tqdm(xlsx_files):\n",
    "       file_path = os.path.join(folder_path, file)\n",
    "       assert os.path.exists(file_path), \"File path is incorrect\"\n",
    "\n",
    "       # Extract year from the filename (before \".xlsx\")\n",
    "       year = int(file.split('.xlsx')[0])\n",
    "\n",
    "       # Extract tables from the PDF file\n",
    "       ncsl_df = pd.read_excel(file_path)\n",
    "\n",
    "       if file != '2021.xlsx':\n",
    "              ncsl_df.columns = ['state', 'total_seats', 'total_senate', 'senate_dem', 'senate_rep',\n",
    "              'senate_other', 'total_house', 'house_dem', 'house_rep', 'house_other',\n",
    "              'legis_control', 'gov_party', 'state_control']\n",
    "              # Drop columns called 'senate_other' and 'house_other'\n",
    "              ncsl_df = ncsl_df.drop(columns=['senate_other', 'house_other'])\n",
    "              # ncsl_df.head()\n",
    "       else:\n",
    "              # Rename columns manually\n",
    "              ncsl_df.columns = ['state', 'total_seats', 'total_senate', 'senate_dem', 'senate_rep',\n",
    "                     'total_house', 'house_dem', 'house_rep', 'legis_control', 'gov_party', 'state_control']\n",
    "              # ncsl_df.head()\n",
    "\n",
    "       # Remove asterisks from state names in the 'State' column\n",
    "       ncsl_df['state'] = ncsl_df['state'].str.replace(r'\\*', '', regex=True)\n",
    "\n",
    "       # Add the extracted year as a new column \n",
    "       ncsl_df['year'] = year \n",
    "\n",
    "       # Append the modified DataFrame to the list\n",
    "       ncsl_dfs.append(ncsl_df)\n",
    "\n",
    "       print(f\"Completed processing {file}\")\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "ncsl = pd.concat(ncsl_dfs, ignore_index=True)\n",
    "\n",
    "print(ncsl.columns)\n",
    "\n",
    "# Add a new column to the DataFrame with the abbreviations\n",
    "ncsl.loc[:, 'state_abbrev'] = ncsl['state'].map(state_to_abbrev)\n",
    "\n",
    "# Recoding the 'gov_party' column to match ICPSR data\n",
    "ncsl['gov_party'] = ncsl['gov_party'].map({'Dem': 1, 'Rep': 2})\n",
    "\n",
    "# Since in ICPSR I only have state abbreviations, I will drop the column containing the full state names\n",
    "ncsl = ncsl.drop(columns=['state'])\n",
    "\n",
    "ncsl = ncsl.dropna(subset=['state_abbrev'])\n",
    "\n",
    "# Reorder the columns to have identifiers first\n",
    "ncsl = ncsl[['year', 'state_abbrev', 'total_seats', 'total_senate', 'senate_dem', \n",
    "'senate_rep', 'total_house', 'house_dem', 'house_rep', 'legis_control', 'gov_party', 'state_control']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncsl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to convert to numeric\n",
    "numeric_cols = [\n",
    "    'total_seats', 'total_senate', 'senate_dem', 'senate_rep', 'total_house', 'house_dem', 'house_rep'\n",
    "]\n",
    "\n",
    "# Convert specified columns to numeric\n",
    "ncsl[numeric_cols] = ncsl[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Verify the conversion\n",
    "print(ncsl[numeric_cols].dtypes)\n",
    "\n",
    "ncsl['dem_upphse'] = ncsl['senate_dem'] / ncsl['total_senate']\n",
    "ncsl['rep_upphse'] = ncsl['senate_rep'] / ncsl['total_senate']\n",
    "ncsl['dem_lowhse'] = ncsl['house_dem'] / ncsl['total_house']\n",
    "ncsl['rep_lowhse'] = ncsl['house_rep'] / ncsl['total_house']\n",
    "ncsl['shr_dem_in_sess'] = (ncsl['senate_dem'] + ncsl['house_dem']) / ncsl['total_seats']\n",
    "ncsl['shr_rep_in_sess'] = (ncsl['senate_rep'] + ncsl['house_rep']) / ncsl['total_seats']\n",
    "\n",
    "ncsl = ncsl.drop(columns=['total_seats', 'total_senate', 'senate_dem', 'senate_rep', 'total_house', 'house_dem', 'house_rep', 'legis_control', 'state_control'])\n",
    "\n",
    "# Now the year is election year\n",
    "ncsl['year'] = ncsl['year'] - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_count_by_year = ncsl.groupby('year')['state_abbrev'].nunique()\n",
    "\n",
    "# Assertions\n",
    "assert (state_count_by_year == 50).all(), \"Not all years have 50 states!\"\n",
    "assert ncsl['year'].min() == 2008, \"The minimum year should be 2008.\"\n",
    "assert ncsl['year'].max() == 2020, \"The maximum year should be 2020.\"\n",
    "assert ncsl['year'].nunique() == 13, \"There should be 12 unique years.\"\n",
    "\n",
    "assert ncsl['gov_party'].nunique() == 2, \"There should be 2 unique parties.\"\n",
    "\n",
    "# Check if all values in 'gov_party' are valid\n",
    "valid_values = {1, 2}  \n",
    "assert ncsl['gov_party'].dropna().isin(valid_values).all(), \"All values in 'gov_party' should be 1, 2, or NaN.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double-check that Klarner and NCSL align for 2009-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncsl_filt = ncsl[(ncsl['year'] <= 2010)]\n",
    "klarner_filt = klarner1[klarner1['year'] >= 2008]\n",
    "\n",
    "print(f\"Years in ncsl: {ncsl_filt['year'].unique()}\")\n",
    "print(f\"Years in klarner: {klarner_filt['year'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(klarner_filt, ncsl_filt, on=['year', 'state_abbrev'], suffixes=('_klarner', '_ncsl'), how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all columns with '_klarner' and '_ncsl' suffixes\n",
    "klarner_cols = [col for col in merged.columns if col.endswith('_klarner')]\n",
    "ncsl_cols = [col.replace('_klarner', '_ncsl') for col in klarner_cols if col.replace('_klarner', '_ncsl') in merged.columns]\n",
    "\n",
    "# Initialize a dictionary to store comparison results\n",
    "comparison_results = {}\n",
    "\n",
    "# Define the error margin\n",
    "error_margin = 0.1\n",
    "\n",
    "# Compare each pair of _klarner and _ncsl columns\n",
    "for klarner_col, ncsl_col in zip(klarner_cols, ncsl_cols):\n",
    "    # Check for mismatched rows within the error margin\n",
    "    mismatched_rows = merged[~np.isclose(merged[klarner_col], merged[ncsl_col], atol=error_margin)]\n",
    "    \n",
    "    # Exclude rows where both are NaN\n",
    "    mismatched_rows = mismatched_rows[\n",
    "        ~(merged[klarner_col].isna() & merged[ncsl_col].isna())\n",
    "    ]\n",
    "    \n",
    "    # Store the mismatches in the dictionary\n",
    "    comparison_results[f\"{klarner_col} vs {ncsl_col}\"] = mismatched_rows[['year', 'state_abbrev', klarner_col, ncsl_col]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At 0.1 error margin, the percentage of mismatches is 0 for all columns except gov_party.\n",
    "col_types = ['gov_party', 'dem_upphse', 'rep_upphse', 'dem_lowhse', 'rep_lowhse', 'shr_dem_in_sess', 'shr_rep_in_sess']\n",
    "mismatch_percentages = []\n",
    "for col_type in col_types:\n",
    "    # Access mismatched data\n",
    "    mismatched_data = comparison_results.get(f'{col_type}_klarner vs {col_type}_ncsl', pd.DataFrame())\n",
    "    \n",
    "    # Calculate mismatch percentage if mismatched_data is not empty\n",
    "    if not mismatched_data.empty:\n",
    "        percentage_mismatch = mismatched_data.shape[0] / merged.shape[0] * 100\n",
    "        mismatch_percentages.append(percentage_mismatch)\n",
    "        print(f\"Percentage of mismatches At 0.01 error margin:\")\n",
    "        print(f\"There are {mismatched_data.shape[0]} {col_type} mismatches.\")\n",
    "        print(f\"So for {col_type} there are {percentage_mismatch:.1f}% of data is mismatched.\")\n",
    "    else:\n",
    "        # Append 0% for columns with no mismatches\n",
    "        mismatch_percentages.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(col_types, mismatch_percentages, color='skyblue')\n",
    "plt.title(\"Percentage of Mismatches at 0.1 Error Margin\", fontsize=14)\n",
    "plt.xlabel(\"Column Types\", fontsize=12)\n",
    "plt.ylabel(\"Mismatch Percentage (%)\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the error margin\n",
    "error_margin = 0.01\n",
    "\n",
    "# Compare each pair of _klarner and _ncsl columns\n",
    "for klarner_col, ncsl_col in zip(klarner_cols, ncsl_cols):\n",
    "    # Check for mismatched rows within the error margin\n",
    "    mismatched_rows = merged[~np.isclose(merged[klarner_col], merged[ncsl_col], atol=error_margin)]\n",
    "    \n",
    "    # Exclude rows where both are NaN\n",
    "    mismatched_rows = mismatched_rows[\n",
    "        ~(merged[klarner_col].isna() & merged[ncsl_col].isna())\n",
    "    ]\n",
    "    \n",
    "    # Store the mismatches in the dictionary\n",
    "    comparison_results[f\"{klarner_col} vs {ncsl_col}\"] = mismatched_rows[['year', 'state_abbrev', klarner_col, ncsl_col]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At 0.01 error margin, the percentage of mismatches is less than 10% for all columns except gov_party.\n",
    "col_types = ['gov_party', 'dem_upphse', 'rep_upphse', 'dem_lowhse', 'rep_lowhse', 'shr_dem_in_sess', 'shr_rep_in_sess']\n",
    "mismatch_percentages = []\n",
    "for col_type in col_types:\n",
    "    # Access mismatched data\n",
    "    mismatched_data = comparison_results.get(f'{col_type}_klarner vs {col_type}_ncsl', pd.DataFrame())\n",
    "    \n",
    "    # Calculate mismatch percentage if mismatched_data is not empty\n",
    "    if not mismatched_data.empty:\n",
    "        percentage_mismatch = mismatched_data.shape[0] / merged.shape[0] * 100\n",
    "        mismatch_percentages.append(percentage_mismatch)\n",
    "        print(f\"Percentage of mismatches At 0.01 error margin:\")\n",
    "        print(f\"There are {mismatched_data.shape[0]} {col_type} mismatches.\")\n",
    "        print(f\"So for {col_type} there are {percentage_mismatch:.1f}% of data is mismatched.\")\n",
    "    else:\n",
    "        # Append 0% for columns with no mismatches\n",
    "        mismatch_percentages.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(col_types, mismatch_percentages, color='skyblue')\n",
    "plt.title(\"Percentage of Mismatches at 0.01 Error Margin\", fontsize=14)\n",
    "plt.xlabel(\"Column Types\", fontsize=12)\n",
    "plt.ylabel(\"Mismatch Percentage (%)\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_results['gov_party_klarner vs gov_party_ncsl'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2009, the Governor of Virginia was Tim Kaine, a member of the Democratic Party. He served as governor from January 14, 2006, to January 16, 2010 thus it is a mistake in ncsl dataset. However 1 mistake in 3 years is great. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatinate all the data\n",
    "Concatinating icpsr_complete (1832-1934), Klarner1 (1935-2010), ncsl (2011-2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncsl = ncsl[ncsl['year'] > 2010]\n",
    "klarner1 = klarner1[klarner1['year'] > 1935]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([icpsr_complete, klarner1, ncsl], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertions\n",
    "assert final_df['year'].min() == 1834, \"The minimum year should be 2008.\"\n",
    "assert final_df['year'].max() == 2020, \"The maximum year should be 2020.\"\n",
    "assert final_df['year'].nunique() == 187, \"There should be 12 unique years.\"\n",
    "\n",
    "assert final_df['gov_party'].nunique() == 2, \"There should be 2 unique parties.\"\n",
    "\n",
    "# Check if all values in 'gov_party' are valid\n",
    "valid_values = {1, 2}  \n",
    "assert final_df['gov_party'].dropna().isin(valid_values).all(), \"All values in 'gov_party' should be 1, 2, or NaN.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statehood_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['year'].value_counts().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df[final_df['year'] == 1974]['state_abbrev'].value_counts().head(2))\n",
    "# Clearly the problem is that we have two Louisianas in 1974."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df[(final_df['state_abbrev'] == 'LA') & (final_df['year'] == 1974)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I cross-references both Louisianas against wikipedia: https://en.wikipedia.org/wiki/Political_party_strength_in_Louisiana, and I can see that both are wrong so I am going to substitute them with the correct version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the two original observations for Louisiana in 1974\n",
    "final_df = final_df[~((final_df['year'] == 1974) & (final_df['state_abbrev'] == 'LA'))]\n",
    "\n",
    "# Total members in Senate and House\n",
    "total_senate = 39\n",
    "total_house = 105\n",
    "total_members = total_senate + total_house\n",
    "\n",
    "# Calculate the new values for shr_dem_in_session and shr_rep_in_session\n",
    "shr_dem_in_sess = (1 * total_senate + 0.961905 * total_house) / total_members\n",
    "shr_rep_in_sess = (0 * total_senate + 0.038095 * total_house) / total_members\n",
    "\n",
    "# Create a new observation for Louisiana in 1974\n",
    "new_observation = pd.DataFrame([{\n",
    "    'year': 1974,\n",
    "    'state_abbrev': 'LA',\n",
    "    'gov_party': 1.0,  # Preserved from the original\n",
    "    'dem_upphse': 1.0,  # Adjusted to 1\n",
    "    'rep_upphse': 0.0,  # Adjusted to 0\n",
    "    'dem_lowhse': 0.961905,  # Preserved from the original\n",
    "    'rep_lowhse': 0.038095,  # Preserved from the original\n",
    "    'shr_dem_in_sess': shr_dem_in_sess,\n",
    "    'shr_rep_in_sess': shr_rep_in_sess\n",
    "}])\n",
    "\n",
    "# Append the new observation to final_df using pd.concat\n",
    "final_df = pd.concat([final_df, new_observation], ignore_index=True)\n",
    "\n",
    "# Display the result for Louisiana in 1974\n",
    "print(final_df[(final_df['year'] == 1974) & (final_df['state_abbrev'] == 'LA')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the index as a column, starting from 1\n",
    "statehood_df1 = statehood_df.reset_index(drop=False)\n",
    "statehood_df1['index'] = statehood_df1['index'] + 1\n",
    "\n",
    "# Sort the DataFrame by 'statehood_year' and 'index' (if not already sorted)\n",
    "statehood_df1 = statehood_df1.sort_values(by=['statehood_year', 'index'])\n",
    "\n",
    "# Drop all rows except the last one for each 'statehood_year'\n",
    "statehood_df1 = statehood_df1.groupby('statehood_year', group_keys=False).tail(1)\n",
    "\n",
    "# Define the full range of years\n",
    "full_years = pd.DataFrame({'statehood_year': range(1787, 2022)})\n",
    "\n",
    "# Merge with the existing DataFrame to include all years\n",
    "statehood_df1 = pd.merge(full_years, statehood_df1, on='statehood_year', how='left')\n",
    "\n",
    "# Forward-fill to propagate the last known index value to missing years\n",
    "statehood_df1['index'] = statehood_df1['index'].ffill()\n",
    "statehood_df1['state_abbrev'] = statehood_df1['state_abbrev'].ffill()\n",
    "\n",
    "# Ensure the DataFrame is sorted by year\n",
    "statehood_df1 = statehood_df1.sort_values(by='statehood_year').reset_index(drop=True)\n",
    "\n",
    "statehood_df1 = statehood_df1.drop(columns=['state_abbrev'])\n",
    "\n",
    "statehood_df1 = statehood_df1.rename(columns={'index': 'state_count', 'statehood_year': 'year'})\n",
    "\n",
    "statehood_df1['year'] = statehood_df1['year'] - 1\n",
    "\n",
    "num_of_states = statehood_df1[statehood_df1['year'] >= 1834]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_states_final = final_df['year'].value_counts()\n",
    "\n",
    "merged_num_of_states = pd.merge(num_of_states, num_of_states_final, on='year', how='outer')\n",
    "\n",
    "merged_num_of_states = merged_num_of_states.rename(\n",
    "    columns={\n",
    "        'state_count': 'num_states_statehood',  \n",
    "        'count': 'num_states_final'           \n",
    "    }\n",
    ")\n",
    "print(merged_num_of_states.head(3))\n",
    "print(merged_num_of_states.columns)\n",
    "\n",
    "# Check if the columns align\n",
    "mismatched_rows = merged_num_of_states[\n",
    "    merged_num_of_states['num_states_statehood'] != merged_num_of_states['num_states_final']\n",
    "]\n",
    "\n",
    "# Display the mismatched rows with all relevant columns\n",
    "print(mismatched_rows.shape[0])\n",
    "print(mismatched_rows[['year', 'num_states_statehood', 'num_states_final']].sort_values(by='year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klarner1_0[(klarner1_0['year'] >= 1948) & (klarner1_0['year'] < 1959)]['year'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klarner_AK_HI = klarner1[klarner1['state_abbrev'].isin(['AK', 'HI'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klarner_AK_HI[(klarner_AK_HI['year'] >= 1948) & (klarner_AK_HI['year'] < 1958)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klarner_AK_HI[klarner_AK_HI['year'] >= 1958].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems like the inclusion of AK and HI for before 1958 is a mistake in the Klarner dataset.\n",
    "\n",
    "# Remove rows for AK and HI before 1958 from the final dataset\n",
    "final_df = final_df[~((final_df['state_abbrev'].isin(['AK', 'HI'])) & (final_df['year'] < 1958))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_states_final = final_df['year'].value_counts()\n",
    "\n",
    "merged_num_of_states = pd.merge(num_of_states, num_of_states_final, on='year', how='outer')\n",
    "\n",
    "merged_num_of_states = merged_num_of_states.rename(\n",
    "    columns={\n",
    "        'state_count': 'num_states_statehood',  \n",
    "        'count': 'num_states_final'           \n",
    "    }\n",
    ")\n",
    "print(merged_num_of_states.head(3))\n",
    "print(merged_num_of_states.columns)\n",
    "\n",
    "assert final_df.duplicated(subset=['year', 'state_abbrev']).sum() == 0, \\\n",
    "    \"Duplicate year-state combinations found in the dataset!\"\n",
    "\n",
    "# Assert that the number of states in the final data aligns with the reality\n",
    "assert (merged_num_of_states['num_states_statehood'] == merged_num_of_states['num_states_final']).all(), \"Mismatch found between num_states_statehood and num_states_final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(os.path.join(intermed_data_dir, 'state_politicalComposition.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with governors data\n",
    "Generated in cleaning_govData.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_data = pd.read_csv(os.path.join(raw_data_dir, \"gov_data_cleaned.csv\"))\n",
    "\n",
    "# Find and display duplicates in gov_data based on all columns\n",
    "duplicates = gov_data[gov_data.duplicated(keep=False)]\n",
    "\n",
    "# Display the duplicates\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicate rows found:\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = pd.read_csv(os.path.join(raw_data_dir, \"state_regions.csv\"))\n",
    "\n",
    "regions.rename(columns={'State Code': 'state_abbrev', 'Region': 'region', 'Division':'division'}, inplace=True)\n",
    "regions = regions.drop(columns=['State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged = pd.merge(final_df, gov_data, on=['year', 'state_abbrev'], how='left', suffixes=('_final', '_gov'))\n",
    "final_merged = pd.merge(final_merged, regions, on='state_abbrev', how='left')\n",
    "final_merged = final_merged.drop(columns=['dem_upphse', 'rep_upphse',\n",
    "       'dem_lowhse', 'rep_lowhse', 'shr_dem_in_sess', 'shr_rep_in_sess'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert final_merged.duplicated(subset=['year', 'state_abbrev']).sum() == 0, \\\n",
    "    \"Duplicate year-state combinations found in the dataset!\"\n",
    "\n",
    "assert final_merged['year'].min() == 1834, \"The minimum year should be 1834.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mismatched rows\n",
    "mismatched_rows = final_merged[final_merged['gov_party_final'] != final_merged['gov_party_gov']]\n",
    "\n",
    "mismatched_rows = mismatched_rows[mismatched_rows['year']<2020]\n",
    "final_df_filt = final_df[final_df['year'] < 2020]\n",
    "\n",
    "# Exclude rows with NA in either column and find mismatched rows\n",
    "mismatched_rows = final_merged[\n",
    "    final_merged['gov_party_gov'].notna() &\n",
    "    (final_merged['gov_party_final'] != final_merged['gov_party_gov']) &\n",
    "    (final_merged['year'] < 2020)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of mismatched rows: {mismatched_rows.shape[0] / final_df.shape[0] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of observations per year in final_df\n",
    "total_year_counts = final_df['year'].value_counts().sort_index()\n",
    "\n",
    "# Count the number of mismatched observations per year\n",
    "mismatched_year_counts = mismatched_rows['year'].value_counts().sort_index()\n",
    "\n",
    "# Calculate the percentage of mismatched rows per year\n",
    "year_percentages = (mismatched_year_counts / total_year_counts) * 100\n",
    "\n",
    "# Plot the percentage of mismatched observations per year\n",
    "plt.bar(year_percentages.index, year_percentages.values, width=0.9)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage of Mismatched Data per Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 25-year bin column in final_df and mismatched_rows\n",
    "final_df['year_bin'] = (final_df['year'] // 25) * 25\n",
    "mismatched_rows['year_bin'] = (mismatched_rows['year'] // 25) * 25\n",
    "\n",
    "# Count total observations per 25-year bin in final_df\n",
    "total_bin_counts = final_df['year_bin'].value_counts().sort_index()\n",
    "\n",
    "# Count mismatched observations per 25-year bin\n",
    "mismatched_bin_counts = mismatched_rows['year_bin'].value_counts().sort_index()\n",
    "\n",
    "# Calculate the percentage of mismatched rows per 25-year bin\n",
    "bin_percentages = (mismatched_bin_counts / total_bin_counts) * 100\n",
    "\n",
    "# Print the overall percentage of mismatched rows\n",
    "overall_percentage = mismatched_rows.shape[0] / final_df.shape[0] * 100\n",
    "print(f\"Percentage of mismatched rows: {overall_percentage:.2f}%\")\n",
    "\n",
    "# Plot the percentage of mismatched observations by 25-year bins\n",
    "plt.bar(bin_percentages.index, bin_percentages.values, width=20)\n",
    "plt.xlabel('Year Bin (25-Year Intervals)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage of Mismatched Data by 25-Year Bins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 25-year bin column in final_df and mismatched_rows\n",
    "final_df['year_bin'] = (final_df['year'] // 25) * 25\n",
    "mismatched_rows['year_bin'] = (mismatched_rows['year'] // 25) * 25\n",
    "\n",
    "# Count total observations per 25-year bin in final_df\n",
    "total_bin_counts = final_df['year_bin'].value_counts().sort_index()\n",
    "\n",
    "# Count mismatched observations per 25-year bin\n",
    "mismatched_bin_counts = mismatched_rows['year_bin'].value_counts().sort_index()\n",
    "\n",
    "# Calculate the percentage of mismatched rows per 25-year bin\n",
    "bin_percentages = (mismatched_bin_counts / total_bin_counts) * 100\n",
    "\n",
    "# Print the overall percentage of mismatched rows\n",
    "overall_percentage = mismatched_rows.shape[0] / final_df.shape[0] * 100\n",
    "print(f\"Overall Percentage of mismatched rows: {overall_percentage:.2f}%\")\n",
    "\n",
    "# Print percentages by 25-year bins\n",
    "print(\"\\nPercentage of mismatched rows by 25-year bins:\")\n",
    "for year_bin, percentage in bin_percentages.items():\n",
    "    print(f\"{year_bin}-{year_bin + 24}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = 'region'\n",
    "assert color in ['region', 'division'], \"color must be 'region' or 'division'\"\n",
    "\n",
    "# Assign colors based on the selected column ('region' or 'division')\n",
    "if color == 'region':\n",
    "    region_colors = {\n",
    "        'South': 'orange', 'West': 'magenta', 'Northeast': 'blue', 'Midwest': 'purple'\n",
    "    }\n",
    "\n",
    "    # Map colors to all states in the regions DataFrame\n",
    "    state_colors = regions.set_index('state_abbrev')['region'].map(region_colors)\n",
    "\n",
    "if color == \"division\":\n",
    "    division_colors = {\n",
    "        'New England': 'blue',\n",
    "        'South Atlantic': 'orange',\n",
    "        'Middle Atlantic': 'green',\n",
    "        'East North Central': 'purple',\n",
    "        'West North Central': 'red',\n",
    "        'East South Central': 'cyan',\n",
    "        'West South Central': 'yellow',\n",
    "        'Pacific': 'magenta',\n",
    "        'Mountain': 'brown'\n",
    "    }\n",
    "\n",
    "    # Map colors to all states in the regions DataFrame\n",
    "    state_colors = regions.set_index('state_abbrev')['division'].map(division_colors)\n",
    "\n",
    "# Count total observations per state in final_df\n",
    "total_state_counts = final_df['state_abbrev'].value_counts()\n",
    "\n",
    "# Count mismatched observations per state\n",
    "mismatched_state_counts = mismatched_rows['state_abbrev'].value_counts()\n",
    "\n",
    "# Calculate the percentage of mismatched rows per state\n",
    "state_percentages = (mismatched_state_counts / total_state_counts) * 100\n",
    "\n",
    "# Ensure plot colors align with the calculated state_percentages\n",
    "plot_colors = state_colors.loc[state_percentages.index]\n",
    "\n",
    "# Plot the percentage of observations per state\n",
    "plt.bar(state_percentages.index, state_percentages.values, color=plot_colors, width=0.5)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage of Mismatched Data per State')\n",
    "plt.xticks(rotation=45, fontsize=5)\n",
    "\n",
    "# Add legend based on the selected color grouping\n",
    "if color == 'division':\n",
    "    handles = [plt.Line2D([0], [0], color=color, lw=4) for color in division_colors.values()]\n",
    "    labels = division_colors.keys()\n",
    "    # Place the legend inside the plot\n",
    "    plt.legend(\n",
    "        handles, labels, title='Division',\n",
    "        loc='upper right',  \n",
    "        bbox_to_anchor=(0.99, 0.99),  \n",
    "        frameon=True \n",
    "    )\n",
    "if color == 'region':\n",
    "    handles = [plt.Line2D([0], [0], color=color, lw=4) for color in region_colors.values()]\n",
    "    labels = region_colors.keys()\n",
    "    # Place the legend inside the plot\n",
    "    plt.legend(\n",
    "        handles, labels, title='Region',\n",
    "        loc='upper right',  \n",
    "        bbox_to_anchor=(0.99, 0.99),  \n",
    "        frameon=True \n",
    "    )\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_mismatched_rows = mismatched_rows[mismatched_rows['state_abbrev'] == 'MA'].copy()\n",
    "MA_final_df = final_df[final_df['state_abbrev'] == 'MA'].copy()\n",
    "\n",
    "print(f\"Percentage of mismatched rows for Massachusetts: {MA_mismatched_rows.shape[0] / MA_final_df.shape[0] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 25-year bin column in final_df and MA_mismatched_rows\n",
    "MA_final_df['year_bin'] = (MA_final_df['year'] // 25) * 25\n",
    "MA_mismatched_rows['year_bin'] = (MA_mismatched_rows['year'] // 25) * 25\n",
    "\n",
    "# Count total observations per 25-year bin in final_df\n",
    "total_bin_counts = MA_final_df['year_bin'].value_counts().sort_index()\n",
    "\n",
    "# Count mismatched observations per 25-year bin\n",
    "mismatched_bin_counts = MA_mismatched_rows['year_bin'].value_counts().sort_index()\n",
    "\n",
    "# Calculate the percentage of mismatched rows per 25-year bin\n",
    "bin_percentages = (mismatched_bin_counts / total_bin_counts) * 100\n",
    "\n",
    "# Print the overall percentage of mismatched rows\n",
    "overall_percentage = MA_mismatched_rows.shape[0] / MA_final_df.shape[0] * 100\n",
    "print(f\"Percentage of mismatched rows: {overall_percentage:.2f}%\")\n",
    "\n",
    "# Plot the percentage of mismatched observations by 25-year bins\n",
    "plt.bar(bin_percentages.index, bin_percentages.values, width=20)\n",
    "plt.xlabel('Year Bin (25-Year Intervals)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage of Mismatched Data by 25-Year Bins for Massachusetts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 25-year bin column in MA_final_df and MA_mismatched_rows\n",
    "MA_final_df['year_bin'] = (MA_final_df['year'] // 25) * 25\n",
    "MA_mismatched_rows['year_bin'] = (MA_mismatched_rows['year'] // 25) * 25\n",
    "\n",
    "# Count total observations per 25-year bin in MA_final_df\n",
    "total_bin_counts = MA_final_df['year_bin'].value_counts().sort_index()\n",
    "\n",
    "# Count mismatched observations per 25-year bin\n",
    "mismatched_bin_counts = MA_mismatched_rows['year_bin'].value_counts().sort_index()\n",
    "\n",
    "# Calculate the percentage of mismatched rows per 25-year bin\n",
    "bin_percentages = (mismatched_bin_counts / total_bin_counts) * 100\n",
    "\n",
    "# Print the overall percentage of mismatched rows\n",
    "overall_percentage = MA_mismatched_rows.shape[0] / MA_final_df.shape[0] * 100\n",
    "print(f\"Overall Percentage of mismatched rows: {overall_percentage:.2f}%\")\n",
    "\n",
    "# Print percentages by 25-year bins\n",
    "print(\"\\nPercentage of mismatched rows by 25-year bins:\")\n",
    "for year_bin, percentage in bin_percentages.items():\n",
    "    print(f\"{year_bin}-{year_bin + 24}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_mismatched_rows.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marcus Morton was affiliated with the Democratic Party. He served as the Governor of Massachusetts during the mid-19th century and was notable for his advocacy of Jacksonian Democratic principles. Morton was elected governor in 1839 and later served another term in 1843."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_mismatched_rows.sample(20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcus_morton_df = MA_mismatched_rows[MA_mismatched_rows['governor'] == 'Marcus Morton']\n",
    "\n",
    "print(f'Marcus Morton appears {marcus_morton_df.shape[0]} times as governor of MA.')\n",
    "\n",
    "print(marcus_morton_df['year'].unique())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Revekka_first_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
